{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0oaV69iaTiC_",
        "bzOMqbshUkck",
        "NG_eYW29Z-Ko",
        "IkeNSNbahS1Q",
        "1RqyiLRPluFm",
        "r0jnE0zqrGwM",
        "lYo71E3WrMiL",
        "2wv-m8-3tUta",
        "-1MZqIzqwc5E",
        "K_YwM-ojxGeC",
        "j2pFu83nyDSG"
      ],
      "authorship_tag": "ABX9TyMJJwPDMhAobMmEaalKPB63",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cseudave/automatic_tops/blob/main/Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping"
      ],
      "metadata": {
        "id": "0oaV69iaTiC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anidb"
      ],
      "metadata": {
        "id": "bzOMqbshUkck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anidb es una página que almacena información de casi todos los animes. Incluso tiene una sección de etiquetas ponderadas para cada serie. Por lo que será necesario BeautifulSoup4"
      ],
      "metadata": {
        "id": "NwF5NNGFUnZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install BeautifulSoup4\n",
        "!pip install cfscrape\n",
        "!pip install cloudscraper"
      ],
      "metadata": {
        "id": "CqekUxhuTjss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cloudscraper será necesario para lidiar con protecciones de algunos sitios"
      ],
      "metadata": {
        "id": "sqAYq4oaUDf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos librerías\n",
        "from bs4 import BeautifulSoup\n",
        "import cloudscraper\n",
        "import requests\n",
        "from cfscrape import create_scraper\n",
        "import urllib.request\n",
        "\n",
        "import random\n",
        "import time \n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "COe_NVaQbbLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos los datos del html de algún sitio usando BeautifulSoup\n",
        "def ingrediente(scraper, url):\n",
        "  response = scraper.get(url)\n",
        "  # Usar BS4 para procesar el contenido \n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  return soup"
      ],
      "metadata": {
        "id": "d-3sKq1qbZCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuramos un scraper para usarlo en chrome\n",
        "scraper = cloudscraper.create_scraper(delay=10, browser='chrome') "
      ],
      "metadata": {
        "id": "1fR2sTsMbWCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se construyen las url para las primeras dos páginas por cada temporada del 2022\n",
        "links = []\n",
        "year = 2022\n",
        "for season in seasons:\n",
        "  for page in pages:\n",
        "    links.append('https://anidb.net/anime/?h=1&noalias=1&orderby.name=1.1&orderby.ucnt=0.2&'+ str(page)+\\\n",
        "              '&season.month='+ str(season) + '&season.year=' + str(year) + '&view=list')"
      ],
      "metadata": {
        "id": "Il5Vb5WnUm5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# De cada página obtendremos los nombres del listado para obtener los urls por cada anime \n",
        "def anidb(db, link):\n",
        "  sopa = ingrediente(scraper, link)\n",
        "  titles = sopa.find_all('td', class_='name main anime')\n",
        "  hrefs = sopa.find_all('td', class_='name main anime')\n",
        "  for title, href in zip(titles, hrefs):\n",
        "    db[title.find('a').text] = 'https://anidb.net/' + href.find('a')['href']\n",
        "  return db"
      ],
      "metadata": {
        "id": "ly4CKrteVGbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for link in links:\n",
        "  db = anidb(db, link)\n",
        "  # Se utiliza una pausa de tiempo aleatorio para no ser bloqueados\n",
        "  time.sleep(random.randint(2, 7))"
      ],
      "metadata": {
        "id": "RJNw24PhW7ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se guardan los datos en una DataFrame\n",
        "df = pd.DataFrame([[key, db[key]] for key in  db.keys()], columns=['anime', 'link'])\n",
        "# Y se guarda la lista de links y nombres de animes\n",
        "df.to_csv('anidb_links22.csv')"
      ],
      "metadata": {
        "id": "NHohyqaYXKBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se busca en la sopa los datos relevantes\n",
        "# En caso de no existir ese dato especifico se guarda uno alternativo\n",
        "def new_dic(sopa):\n",
        "  try:\n",
        "    table = sopa.find_all('table')[0].find_all('td')\n",
        "  except:\n",
        "    print(\"Bloqueo\")\n",
        "    return 'stop'\n",
        "  \n",
        "  starts = sopa.find_all('span', class_='weight')\n",
        "  tagnames = sopa.find_all('span', class_='tagname')\n",
        "  nd = {}\n",
        "\n",
        "  nd['name'] = table[0].find('span', itemprop='name').text\n",
        "  nd['img'] = img = sopa.find('img', itemprop='image')['src']\n",
        "  try:\n",
        "    nd['name_en'] = table[1].find('label', itemprop='alternateName').text\n",
        "  except:\n",
        "    nd['name_en'] = table[0].find('span', itemprop='name').text\n",
        "  try:\n",
        "    nd['name_jp'] = table[2].find('label', itemprop='alternateName').text\n",
        "  except: \n",
        "    nd['name_jp'] = table[0].find('span', itemprop='name').text\n",
        "  nd['type'] = table[3].text.split(',')[0]\n",
        "  try:\n",
        "    nd['episodes'] = table[3].text.split(',')[1]\n",
        "  except:\n",
        "    nd['episodes'] = 1\n",
        "  nd['start'] = table[4].text.split(' until ')[0]\n",
        "  try:\n",
        "    nd['end'] = table[4].text.split(' until ')[1]\n",
        "  except:\n",
        "    nd['end'] = table[4].text.split(' until ')[0]\n",
        "  nd['season'] = table[5].text.split(' ')[0]\n",
        "  try:\n",
        "    nd['year'] = table[5].text.split(' ')[1]\n",
        "  except:\n",
        "    nd['year'] =  None\n",
        "  genre = []\n",
        "  for line in table[6].find_all('span', itemprop='genre'):\n",
        "    genre.append(line.text)\n",
        "  nd['genre'] = genre\n",
        "  link_ex = []\n",
        "  for line in table[7].find_all('a'):\n",
        "    link_ex.append(line['href'])\n",
        "  nd['link_ex'] = link_ex\n",
        "  nd['rating'] = table[8].text.split(' ')[0]\n",
        "  try:\n",
        "    nd['nrating'] = table[8].text.split(' ')[1]\n",
        "  except:\n",
        "    nd['nrating'] = None\n",
        "  nd['average'] = table[9].text.split(' ')[0]\n",
        "  try: \n",
        "    nd['naverage'] = table[9].text.split(' ')[1]\n",
        "  except:\n",
        "    nd['naverage'] = None\n",
        "  try:\n",
        "    nd['rrating'] = table[10].text.split(' ')[0]\n",
        "  except:\n",
        "    nd['rrating'] = None\n",
        "  try:\n",
        "    nd['nrrating'] = table[10].text.split(' ')[1]\n",
        "  except:\n",
        "    nd['nrrating'] = None\n",
        "  star_dic = {}\n",
        "  for i in range(len(starts) - 2):\n",
        "    star_dic[tagnames[i + len(nd['genre'])].text] = starts[i].text.replace('\\n', '')\n",
        "  nd['tags'] = star_dic\n",
        "  return nd"
      ],
      "metadata": {
        "id": "Y15bU2rHXicQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos librerías que nos permitirán ver el avance\n",
        "from ipywidgets import IntProgress\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "VYK4XdHXXysD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los links\n",
        "df = pd.read_csv('anidb_links22.csv')\n",
        "urls = df['link']"
      ],
      "metadata": {
        "id": "n9TC5N6bZfn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos db22.csv para repetir el scrap\n",
        "# Porque en ocaciones se bloquea nuestra IP\n",
        "# Inicialmente no existe este archivo\n",
        "try:\n",
        "  df = pd.read_csv('db22.csv')\n",
        "  data = df.to_dict(orient='records')\n",
        "except:\n",
        "  data = {}"
      ],
      "metadata": {
        "id": "KH4PBKQzYsHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "progress = IntProgress()\n",
        "display(progress)\n",
        "scraper = cloudscraper.create_scraper(delay=10, browser='chrome') \n",
        "\n",
        "# Obtenemos los datos deseados de cada url faltante\n",
        "# Comenzando desde donde fuimos bloqueados\n",
        "for i in range(len(data) - 1, len(urls)):\n",
        "  url = urls[i]\n",
        "  sopa = ingrediente(scraper, url)\n",
        "  p = new_dic(sopa)\n",
        "  if p =='stop':\n",
        "    break\n",
        "  prueba.append(p)\n",
        "  time.sleep(random.uniform(0, 1))\n",
        "  progress.value = i"
      ],
      "metadata": {
        "id": "bVovXMC8ZQGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el dataframe para guardarlo\n",
        "df_db = pd.DataFrame.from_dict(prueba)\n",
        "df_db.to_csv('db22.csv', index=False)\n",
        "df_db.to_excel('db22.xlsx', index=False)"
      ],
      "metadata": {
        "id": "xMD2WySVZzXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2gScgmOqZ87T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anilist"
      ],
      "metadata": {
        "id": "NG_eYW29Z-Ko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternativamente se usará los datos de otra página, o bien, comparando el rendimiento de ambos casos y poder elegir la mejor opción"
      ],
      "metadata": {
        "id": "t-qHQwDZamoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debido a cambios en el sistema operativo de las computadoras de google colab es necesario correr el siguiente script para poder utilizar Selenium"
      ],
      "metadata": {
        "id": "acrecDmXa2b0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Ubuntu no longer distributes chromium-browser outside of snap\n",
        "#\n",
        "# Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap\n",
        "\n",
        "# Add debian buster\n",
        "cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n",
        "EOF\n",
        "\n",
        "# Add keys\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
        "\n",
        "apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n",
        "apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n",
        "apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n",
        "\n",
        "# Prefer debian repo for chromium* packages only\n",
        "# Note the double-blank lines between entries\n",
        "cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n",
        "Package: *\n",
        "Pin: release a=eoan\n",
        "Pin-Priority: 500\n",
        "\n",
        "\n",
        "Package: *\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 300\n",
        "\n",
        "\n",
        "Package: chromium*\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 700\n",
        "EOF\n",
        "\n",
        "# Install chromium and chromium-driver\n",
        "apt-get update\n",
        "apt-get install chromium chromium-driver\n",
        "\n",
        "# Install selenium\n",
        "pip install selenium"
      ],
      "metadata": {
        "id": "PqEvF2jdZ_W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos librerías \n",
        "import re\n",
        "import numpy as np\n",
        "from numpy import arange\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import csv\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "T5vkwwk9bKpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_anilist(url):\n",
        "  # Se configura el driver para usar selenium\n",
        "  options = Options()\n",
        "  options.add_argument(\"--headless\")\n",
        "  options.add_argument(\"--no-sandbox\")\n",
        "  options.headless = True\n",
        "  driver = webdriver.Chrome(\"/usr/bin/chromedriver\", options=options)\n",
        "  driver.get(url)\n",
        "  driver.maximize_window()\n",
        "\n",
        "  time.sleep(random.uniform(3, 4))\n",
        "\n",
        "  # Se copia el contenido de la url como si se hiciera con un mouse\n",
        "  data = driver.find_element(By.XPATH, \"/html/body\").text\n",
        "  driver.close()\n",
        "\n",
        "\n",
        "  return data "
      ],
      "metadata": {
        "id": "OUZjwv_Hb9pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debido a que todas las páginas comparten una estructura similar, se puede utilizar algunas palabras como referencia para obtener los datos deseados"
      ],
      "metadata": {
        "id": "hL-lM64KcSXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encontramos las marcas claves\n",
        "marks = ['Add to List',\n",
        " 'Social',\n",
        "'Format',\n",
        "'Episodes',\n",
        "'Episode Duration',\n",
        "'Status',\n",
        "#'Start Date',\n",
        "#'End Date',\n",
        "'Season',\n",
        "'Average Score',\n",
        "'Mean Score',\n",
        "'Popularity',\n",
        "'Favorites',\n",
        "'Studios',\n",
        "'Producers',\n",
        "'Source',\n",
        "'Hashtag',\n",
        "'Genres',\n",
        "'Romaji',\n",
        "'English',\n",
        "'Native',\n",
        "'Synonyms',\n",
        "'Tags',\n",
        "'External & Streaming links',\n",
        "'Relations',\n",
        "'Characters',\n",
        "'Staff',\n",
        "'Status Distribution',\n",
        "'Score Distribution',\n",
        "'Trailer',\n",
        "'Recommendations',\n",
        "'ThreadsCreate New Thread'\n",
        " ]"
      ],
      "metadata": {
        "id": "xKvvdHKQb1_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos el texto copiado y según las marcas clasificamos los campos con los datos deseados\n",
        "def to_dict(prueba, marks):\n",
        "  nmarks = []\n",
        "  prueba = prueba.split('\\n')\n",
        "  try:\n",
        "    if prueba.index('Overview')  & prueba.index('Stats') < 50:\n",
        "      del prueba[prueba.index('Overview'):prueba.index('Stats') + 1] \n",
        "  except:\n",
        "    None\n",
        "  for mark in marks:\n",
        "    if mark in prueba:\n",
        "      nmarks.append(mark)\n",
        "  cuts = [prueba.index(x) for x in nmarks]\n",
        "  valores = [prueba[cuts[i]+1:cuts[i+1]] for i in range(0, len(cuts) - 1)]\n",
        "  ndict = {}\n",
        "  for x, y in zip(nmarks, valores):\n",
        "    ndict[x] = y\n",
        "  return ndict"
      ],
      "metadata": {
        "id": "6cm5rJH3cjNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función anterior solo requiere la lista de urls seleccionados. Por lo que se crea a mano una hoja de calculo con ellos, llamado anilinks.xlsx"
      ],
      "metadata": {
        "id": "hHrY4QzRdudg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('anilinks.xlsx', sheet_name='Hoja 1' )"
      ],
      "metadata": {
        "id": "qioONaqIeDnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def more_anilist(links, db, marks):\n",
        "  for i in range(len(db), len(links)):\n",
        "    data = get_anilist(links[i])\n",
        "    db.append(to_dict(data, marks))\n",
        "    print(i, links[i])\n",
        "  return db"
      ],
      "metadata": {
        "id": "dXm3kdKBe0BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se crea una lista vacia para agregar los diccionarios con los datos de cada link\n",
        "db = []\n",
        "db = more_anilist(links, db, marks)"
      ],
      "metadata": {
        "id": "3TMIzUn-fBzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# En caso de buscar solo un par de links, por ejemplo\n",
        "# links = [\n",
        "# 'https://anilist.co/anime/113717/Ousama-Ranking']"
      ],
      "metadata": {
        "id": "5S8H1dCcekij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# En caso de agregar manualmente un registro:\n",
        "# Ejemplo reducido de cómo luce el texto copiado\n",
        "\n",
        "data = '''\n",
        "\n",
        "Add to List\n",
        "Akiba Meido sensou \n",
        "Akihabara is the center of the universe for the coolest hobbies and quirkiest amusements. In the spring of 1999, bright-eyed Nagomi Wahira moves there with dreams of joining a maid café. She quickly dons an apron at café Ton Tokoton, AKA the Pig Hut. But adjusting to life in bustling Akihabara isn’t as easy as serving tea and delighting customers. Paired with the dour Ranko who never seems to smile, Nagomi must do her best to elevate the Pig Hut over all other maid cafés vying for top ranking. Along the way she’ll slice out a place for herself amid the frills and thrills of life at the Pig Hut. Just when Nagomi’s dreams are within her grasp, she discovers not everything is as it seems amid the maid cafés of Akihabara.\n",
        "\n",
        " #45 Highest Rated 2022\n",
        " #62 Most Popular 2022\n",
        "Format\n",
        "TV\n",
        "Episodes\n",
        "12\n",
        "Episode Duration\n",
        "24 mins\n",
        "Status\n",
        "Finished\n",
        "Start Date\n",
        "Oct 7, 2022\n",
        "...\n",
        "Recommendations\n",
        "View All Recommendations\n",
        "'''\n",
        "\n",
        "#Traducimos el texto a un diccionario\n",
        "db = [to_dict(data, marks)]\n",
        "#Para poder agregarlo a los registros\n",
        "db = more_anilist(links, db, marks)\n"
      ],
      "metadata": {
        "id": "wGo0JI3efnPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tomamos la base de datos y agregamos el nuevo anime\n",
        "db_new = pd.DataFrame(db)\n",
        "names = []\n",
        "for text in db_new['Add to List']:\n",
        "  names.append(text[0])\n",
        "db_new.insert(0, 'name', names)\n",
        "db_old = pd.read_csv('anilist22_raw.csv')"
      ],
      "metadata": {
        "id": "gqq0bdhmgyKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_full = pd.concat([db_old, db_new], axis=0)\n",
        "db_full.to_csv('anilist22_raw.csv', index=False)"
      ],
      "metadata": {
        "id": "TySChqbbhNj-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
