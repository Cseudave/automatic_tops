{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0oaV69iaTiC_",
        "bzOMqbshUkck",
        "NG_eYW29Z-Ko",
        "IkeNSNbahS1Q",
        "1RqyiLRPluFm",
        "r0jnE0zqrGwM",
        "lYo71E3WrMiL",
        "2wv-m8-3tUta",
        "-1MZqIzqwc5E",
        "K_YwM-ojxGeC",
        "j2pFu83nyDSG"
      ],
      "authorship_tag": "ABX9TyNbf+vxKCHmmRwdb8fnDJvR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cseudave/automatic_tops/blob/main/01_2_Web_Scraping_anidb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping"
      ],
      "metadata": {
        "id": "0oaV69iaTiC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anidb"
      ],
      "metadata": {
        "id": "bzOMqbshUkck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anidb es una página que almacena información de casi todos los animes. Incluso tiene una sección de etiquetas ponderadas para cada serie. Por lo que será necesario BeautifulSoup4"
      ],
      "metadata": {
        "id": "NwF5NNGFUnZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install BeautifulSoup4\n",
        "!pip install cfscrape\n",
        "!pip install cloudscraper"
      ],
      "metadata": {
        "id": "CqekUxhuTjss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cloudscraper será necesario para lidiar con protecciones de algunos sitios"
      ],
      "metadata": {
        "id": "sqAYq4oaUDf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos librerías\n",
        "from bs4 import BeautifulSoup\n",
        "import cloudscraper\n",
        "import requests\n",
        "from cfscrape import create_scraper\n",
        "import urllib.request\n",
        "\n",
        "import random\n",
        "import time \n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "COe_NVaQbbLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos los datos del html de algún sitio usando BeautifulSoup\n",
        "def ingrediente(scraper, url):\n",
        "  response = scraper.get(url)\n",
        "  # Usar BS4 para procesar el contenido \n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  return soup"
      ],
      "metadata": {
        "id": "d-3sKq1qbZCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuramos un scraper para usarlo en chrome\n",
        "scraper = cloudscraper.create_scraper(delay=10, browser='chrome') "
      ],
      "metadata": {
        "id": "1fR2sTsMbWCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se construyen las url para las primeras dos páginas por cada temporada del 2022\n",
        "links = []\n",
        "year = 2022\n",
        "for season in seasons:\n",
        "  for page in pages:\n",
        "    links.append('https://anidb.net/anime/?h=1&noalias=1&orderby.name=1.1&orderby.ucnt=0.2&'+ str(page)+\\\n",
        "              '&season.month='+ str(season) + '&season.year=' + str(year) + '&view=list')"
      ],
      "metadata": {
        "id": "Il5Vb5WnUm5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# De cada página obtendremos los nombres del listado para obtener los urls por cada anime \n",
        "def anidb(db, link):\n",
        "  sopa = ingrediente(scraper, link)\n",
        "  titles = sopa.find_all('td', class_='name main anime')\n",
        "  hrefs = sopa.find_all('td', class_='name main anime')\n",
        "  for title, href in zip(titles, hrefs):\n",
        "    db[title.find('a').text] = 'https://anidb.net/' + href.find('a')['href']\n",
        "  return db"
      ],
      "metadata": {
        "id": "ly4CKrteVGbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for link in links:\n",
        "  db = anidb(db, link)\n",
        "  # Se utiliza una pausa de tiempo aleatorio para no ser bloqueados\n",
        "  time.sleep(random.randint(2, 7))"
      ],
      "metadata": {
        "id": "RJNw24PhW7ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se guardan los datos en una DataFrame\n",
        "df = pd.DataFrame([[key, db[key]] for key in  db.keys()], columns=['anime', 'link'])\n",
        "# Y se guarda la lista de links y nombres de animes\n",
        "df.to_csv('anidb_links22.csv')"
      ],
      "metadata": {
        "id": "NHohyqaYXKBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se busca en la sopa los datos relevantes\n",
        "# En caso de no existir ese dato especifico se guarda uno alternativo\n",
        "def new_dic(sopa):\n",
        "  try:\n",
        "    table = sopa.find_all('table')[0].find_all('td')\n",
        "  except:\n",
        "    print(\"Bloqueo\")\n",
        "    return 'stop'\n",
        "  \n",
        "  starts = sopa.find_all('span', class_='weight')\n",
        "  tagnames = sopa.find_all('span', class_='tagname')\n",
        "  nd = {}\n",
        "\n",
        "  nd['name'] = table[0].find('span', itemprop='name').text\n",
        "  nd['img'] = img = sopa.find('img', itemprop='image')['src']\n",
        "  try:\n",
        "    nd['name_en'] = table[1].find('label', itemprop='alternateName').text\n",
        "  except:\n",
        "    nd['name_en'] = table[0].find('span', itemprop='name').text\n",
        "  try:\n",
        "    nd['name_jp'] = table[2].find('label', itemprop='alternateName').text\n",
        "  except: \n",
        "    nd['name_jp'] = table[0].find('span', itemprop='name').text\n",
        "  nd['type'] = table[3].text.split(',')[0]\n",
        "  try:\n",
        "    nd['episodes'] = table[3].text.split(',')[1]\n",
        "  except:\n",
        "    nd['episodes'] = 1\n",
        "  nd['start'] = table[4].text.split(' until ')[0]\n",
        "  try:\n",
        "    nd['end'] = table[4].text.split(' until ')[1]\n",
        "  except:\n",
        "    nd['end'] = table[4].text.split(' until ')[0]\n",
        "  nd['season'] = table[5].text.split(' ')[0]\n",
        "  try:\n",
        "    nd['year'] = table[5].text.split(' ')[1]\n",
        "  except:\n",
        "    nd['year'] =  None\n",
        "  genre = []\n",
        "  for line in table[6].find_all('span', itemprop='genre'):\n",
        "    genre.append(line.text)\n",
        "  nd['genre'] = genre\n",
        "  link_ex = []\n",
        "  for line in table[7].find_all('a'):\n",
        "    link_ex.append(line['href'])\n",
        "  nd['link_ex'] = link_ex\n",
        "  nd['rating'] = table[8].text.split(' ')[0]\n",
        "  try:\n",
        "    nd['nrating'] = table[8].text.split(' ')[1]\n",
        "  except:\n",
        "    nd['nrating'] = None\n",
        "  nd['average'] = table[9].text.split(' ')[0]\n",
        "  try: \n",
        "    nd['naverage'] = table[9].text.split(' ')[1]\n",
        "  except:\n",
        "    nd['naverage'] = None\n",
        "  try:\n",
        "    nd['rrating'] = table[10].text.split(' ')[0]\n",
        "  except:\n",
        "    nd['rrating'] = None\n",
        "  try:\n",
        "    nd['nrrating'] = table[10].text.split(' ')[1]\n",
        "  except:\n",
        "    nd['nrrating'] = None\n",
        "  star_dic = {}\n",
        "  for i in range(len(starts) - 2):\n",
        "    star_dic[tagnames[i + len(nd['genre'])].text] = starts[i].text.replace('\\n', '')\n",
        "  nd['tags'] = star_dic\n",
        "  return nd"
      ],
      "metadata": {
        "id": "Y15bU2rHXicQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos librerías que nos permitirán ver el avance\n",
        "from ipywidgets import IntProgress\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "VYK4XdHXXysD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los links\n",
        "df = pd.read_csv('anidb_links22.csv')\n",
        "urls = df['link']"
      ],
      "metadata": {
        "id": "n9TC5N6bZfn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos db22.csv para repetir el scrap\n",
        "# Porque en ocaciones se bloquea nuestra IP\n",
        "# Inicialmente no existe este archivo\n",
        "try:\n",
        "  df = pd.read_csv('db22.csv')\n",
        "  data = df.to_dict(orient='records')\n",
        "except:\n",
        "  data = {}"
      ],
      "metadata": {
        "id": "KH4PBKQzYsHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "progress = IntProgress()\n",
        "display(progress)\n",
        "scraper = cloudscraper.create_scraper(delay=10, browser='chrome') \n",
        "\n",
        "# Obtenemos los datos deseados de cada url faltante\n",
        "# Comenzando desde donde fuimos bloqueados\n",
        "for i in range(len(data) - 1, len(urls)):\n",
        "  url = urls[i]\n",
        "  sopa = ingrediente(scraper, url)\n",
        "  p = new_dic(sopa)\n",
        "  if p =='stop':\n",
        "    break\n",
        "  prueba.append(p)\n",
        "  time.sleep(random.uniform(0, 1))\n",
        "  progress.value = i"
      ],
      "metadata": {
        "id": "bVovXMC8ZQGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el dataframe para guardarlo\n",
        "df_db = pd.DataFrame.from_dict(prueba)\n",
        "df_db.to_csv('db22.csv', index=False)\n",
        "df_db.to_excel('db22.xlsx', index=False)"
      ],
      "metadata": {
        "id": "xMD2WySVZzXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2gScgmOqZ87T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
